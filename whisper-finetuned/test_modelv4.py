from transformers import WhisperForConditionalGeneration, WhisperProcessor
import torch
import soundfile as sf
import yake
import re
from collections import Counter

# Model ve tokenizer yollarÄ±
model_dir = "./checkpoint-5"
audio_path = "chunk_1.wav"

# Model ve processor yÃ¼kle
print("Model yÃ¼kleniyor...")
processor = WhisperProcessor.from_pretrained(model_dir)
model = WhisperForConditionalGeneration.from_pretrained(model_dir)

# Ses dosyasÄ±nÄ± oku
audio_input, sample_rate = sf.read(audio_path)
# Stereo ise mono'ya Ã§evir
if len(audio_input.shape) > 1:
    audio_input = audio_input.mean(axis=1)
# EÄŸer Ã¶rnekleme hÄ±zÄ± 16000 deÄŸilse yeniden Ã¶rnekle
target_sample_rate = 16000
if sample_rate != target_sample_rate:
    from scipy.signal import resample
    import numpy as np
    num_samples = int(len(audio_input) * target_sample_rate / sample_rate)
    audio_input = resample(audio_input, num_samples)
    sample_rate = target_sample_rate

# 30 saniyelik segmentlere bÃ¶l ve sÄ±rayla transkribe et
segment_duration = 30  # saniye
segment_samples = target_sample_rate * segment_duration
total_samples = len(audio_input)
total_duration = total_samples / target_sample_rate

print(f"Toplam ses uzunluÄŸu: {total_duration:.2f} saniye")
print(f"Segment sayÄ±sÄ±: {int(total_samples / segment_samples) + (1 if total_samples % segment_samples != 0 else 0)}")
print("="*70)

all_transcriptions = []
for i in range(0, total_samples, segment_samples):
    start = i
    end = min(i + segment_samples, total_samples)
    segment = audio_input[start:end]
    
    if len(segment) == 0:
        continue
        
    print(f"Ä°ÅŸleniyor: {start/target_sample_rate:.1f}-{end/target_sample_rate:.1f} saniye arasÄ±...")
    
    inputs = processor(segment, sampling_rate=sample_rate, return_tensors="pt")
    with torch.no_grad():
        predicted_ids = model.generate(inputs.input_features)
    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    all_transcriptions.append(transcription)

# TÃ¼m transkripsiyon metnini birleÅŸtir
full_text = " ".join(all_transcriptions)

print("\n" + "="*70)
print("TRANSKRÄ°PSÄ°YON:")
print("="*70)
for idx, t in enumerate(all_transcriptions):
    print(f"[{idx+1}. parÃ§a] {t}")

# NLP Ä°ÅŸlemleri - GeliÅŸmiÅŸ Versiyon
print("\n" + "="*70)
print("GELÄ°ÅMÄ°Å NLP ANALÄ°ZÄ°:")
print("="*70)

# 1. CÃ¼mle SkorlamalÄ± Ã–zet (Extractive Summarization)
def advanced_extract_summary(text, num_sentences=2):
    """CÃ¼mleleri skorlayarak en Ã¶nemli cÃ¼mleleri seÃ§"""
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip() and len(s.split()) > 3]
    
    if len(sentences) <= num_sentences:
        return ". ".join(sentences) + "."
    
    # Her cÃ¼mlenin skorunu hesapla (kelime frekansÄ± bazlÄ±)
    words = re.findall(r'\b\w+\b', text.lower())
    word_freq = Counter(words)
    
    # TÃ¼rkÃ§e stopwords
    stopwords = {
        've', 'veya', 'ile', 'bu', 'ÅŸu', 'o', 'bir', 'ama', 'ancak', 
        'fakat', 'Ã§Ã¼nkÃ¼', 'iÃ§in', 'gibi', 'kadar', 'daha', 'en', 'Ã§ok',
        'az', 'ne', 'nasÄ±l', 'neden', 'niÃ§in', 'nerede', 'kim', 'hangi',
        'mi', 'mu', 'mÄ±', 'mÃ¼', 'da', 'de', 'ta', 'te', 'ki', 'ise'
    }
    
    sentence_scores = {}
    for idx, sentence in enumerate(sentences):
        score = 0
        words_in_sentence = re.findall(r'\b\w+\b', sentence.lower())
        for word in words_in_sentence:
            if word not in stopwords and len(word) > 3:
                score += word_freq.get(word, 0)
        
        # CÃ¼mle uzunluÄŸunu normalize et
        if len(words_in_sentence) > 0:
            sentence_scores[idx] = score / len(words_in_sentence)
    
    # En yÃ¼ksek skorlu cÃ¼mleleri seÃ§ (orijinal sÄ±ralamayÄ± koru)
    top_sentence_indices = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]
    top_sentence_indices = sorted([idx for idx, score in top_sentence_indices])
    
    summary_sentences = [sentences[idx] for idx in top_sentence_indices]
    return ". ".join(summary_sentences) + "."

# 2. YAKE ile Anahtar Kelime Ã‡Ä±karma
def extract_keywords_yake(text, top_n=15):
    """YAKE algoritmasÄ± ile anahtar kelimeleri Ã§Ä±kar"""
    # YAKE parametreleri
    language = "tr"  # TÃ¼rkÃ§e
    max_ngram_size = 2  # Tek kelime ve iki kelimeli ifadeler
    deduplication_threshold = 0.9
    num_of_keywords = top_n
    
    custom_kw_extractor = yake.KeywordExtractor(
        lan=language,
        n=max_ngram_size,
        dedupLim=deduplication_threshold,
        top=num_of_keywords,
        features=None
    )
    
    keywords = custom_kw_extractor.extract_keywords(text)
    return keywords

# 3. Basit TF-IDF benzeri yÃ¶ntem (yedek)
def extract_keywords_simple(text, top_n=10):
    """Basit frekans bazlÄ± anahtar kelime Ã§Ä±karma"""
    turkish_stopwords = {
        've', 'veya', 'ile', 'bu', 'ÅŸu', 'o', 'bir', 'ama', 'ancak', 
        'fakat', 'Ã§Ã¼nkÃ¼', 'iÃ§in', 'gibi', 'kadar', 'daha', 'en', 'Ã§ok',
        'az', 'ne', 'nasÄ±l', 'neden', 'niÃ§in', 'nerede', 'kim', 'hangi',
        'mi', 'mu', 'mÄ±', 'mÃ¼', 'da', 'de', 'ta', 'te', 'ya', 'ki',
        'ise', 'eÄŸer', 'olarak', 'Ã¼zere', 'dolayÄ±', 'hem', 'yani'
    }
    
    words = re.findall(r'\b\w+\b', text.lower())
    words = [w for w in words if w not in turkish_stopwords and len(w) > 3]
    word_freq = Counter(words)
    return word_freq.most_common(top_n)

# Ã–zet oluÅŸtur
print("\nğŸ“ Ã–ZET (2 en Ã¶nemli cÃ¼mle):")
summary = advanced_extract_summary(full_text, num_sentences=2)
print(f"{summary}")

# YAKE ile anahtar kelimeleri Ã§Ä±kar
print(f"\nğŸ”‘ ANAHTAR KELÄ°MELER (YAKE AlgoritmasÄ±):")
try:
    keywords_yake = extract_keywords_yake(full_text, top_n=15)
    for idx, (keyword, score) in enumerate(keywords_yake, 1):
        print(f"  {idx}. {keyword} (skor: {score:.4f})")
except Exception as e:
    print(f"  YAKE hatasÄ±: {e}")
    print("\nğŸ”‘ ANAHTAR KELÄ°MELER (Basit YÃ¶ntem):")
    keywords_simple = extract_keywords_simple(full_text, top_n=10)
    for idx, (word, freq) in enumerate(keywords_simple, 1):
        print(f"  {idx}. {word} ({freq} kez)")

# Metin Ä°statistikleri
word_count = len(full_text.split())
char_count = len(full_text)
sentence_count = len(re.split(r'[.!?]+', full_text))
unique_words = len(set(re.findall(r'\b\w+\b', full_text.lower())))

print(f"\nğŸ“Š DETAYLI Ä°STATÄ°STÄ°KLER:")
print(f"  â€¢ Toplam kelime sayÄ±sÄ±: {word_count}")
print(f"  â€¢ Benzersiz kelime sayÄ±sÄ±: {unique_words}")
print(f"  â€¢ Kelime Ã§eÅŸitliliÄŸi: {unique_words/word_count*100:.1f}%")
print(f"  â€¢ Toplam karakter sayÄ±sÄ±: {char_count}")
print(f"  â€¢ CÃ¼mle sayÄ±sÄ±: {sentence_count}")
print(f"  â€¢ Ortalama kelime/cÃ¼mle: {word_count/max(sentence_count, 1):.1f}")
print(f"  â€¢ Segment sayÄ±sÄ±: {len(all_transcriptions)}")
print(f"  â€¢ Ses sÃ¼resi: {total_duration:.2f} saniye ({total_duration/60:.1f} dakika)")

# SonuÃ§larÄ± dosyaya kaydet
output_file = audio_path.replace('.wav', '_analysis.txt')
with open(output_file, 'w', encoding='utf-8') as f:
    f.write("="*70 + "\n")
    f.write("TRANSKRÄ°PSÄ°YON ANALÄ°ZÄ°\n")
    f.write("="*70 + "\n\n")
    
    f.write("Ã–ZET:\n")
    f.write(summary + "\n\n")
    
    f.write("ANAHTAR KELÄ°MELER:\n")
    try:
        for idx, (keyword, score) in enumerate(keywords_yake, 1):
            f.write(f"  {idx}. {keyword}\n")
    except:
        for idx, (word, freq) in enumerate(keywords_simple, 1):
            f.write(f"  {idx}. {word}\n")
    
    f.write("\n" + "="*70 + "\n")
    f.write("TAM TRANSKRÄ°PSÄ°YON:\n")
    f.write("="*70 + "\n")
    f.write(full_text)

print(f"\nğŸ’¾ Analiz raporu kaydedildi: {output_file}")
